# 爬虫基础知识

---

[toc]

## 爬虫数据采集分类

![Snipaste_2020-02-12_00-12-24](F:\code2\爬虫\note\src\Snipaste_2020-02-12_00-12-24.png)





## robot协议

​	在大多数网站根目录下都有的一个`robots.txt`文件，如：

```
https://www.taobao.com/robots.txt
```

​	该文件说明：什么爬虫对什么内容可以爬，什么不能爬。

> 附：其中`$`是首页的意思





## requests库

调用方法

```python
import requests
import json

url = "https://www.taobao.com"

res = requests.get(url=url)
print(res.text)
```

> 其中方法不止get，还有post等
>
> res属性也有text,encording,headers,cookies,status_code等 





## 正则表达式re库

### 常用

![Snipaste_2020-02-12_10-51-44](F:\code2\爬虫\note\src\Snipaste_2020-02-12_10-51-44.png)

>一些注意事项：
>
>1. 区间内字符一般失去意义
>2. 区间中若要匹配 ”-“ 则要将 ”-“ 丢在最后面，不然会被当成范围指定
>3. \\b 和 \\s的区别: 
> - \\b可以匹配candy-cash中的candy，而\\s不能，此外还能匹配candy$crash这种非数字和字母分割的
> - \\s会将空白字符也给匹配进去
> - \\b放在开头也可以匹配文章开头的单词，如\\bCandy可以匹配文章开头的Candy
>4. 转义字符只要一个就行了如    \\\.   就将   .   恢复原来意思了
>5. 用`.*?`实现**非贪婪匹配**，默认的`*?`是贪婪匹配的



### 核心函数

Python提供了re模块来支持正则表达式相关操作，下面是re模块中的核心函数。

| 函数                                             | 说明                                                         |
| ------------------------------------------------ | ------------------------------------------------------------ |
| compile(pattern, flags=0)                        | 编译正则表达式返回正则表达式对象                             |
| **match**(pattern, string, flags=0)              | 用正则表达式匹配字符串 成功返回匹配对象 否则返回None,**注意与search（）不同的是会从头开始匹配，头没有匹配成功就是匹配失败**，且只匹配当前行，除非指定参数 |
| **search**(pattern, string, flags=0)             | 搜索字符串中第一次出现正则表达式的模式 成功返回匹配对象 否则返回None，**从所有行匹配，但只匹配一行** |
| split(pattern, string, maxsplit=0, flags=0)      | 用正则表达式指定的模式分隔符拆分字符串 返回列表              |
| **sub**(pattern, repl, string, count=0, flags=0) | 用指定的字符串**替换**原字符串中与正则表达式匹配的模式 可以用count指定替换的次数 |
| fullmatch(pattern, string, flags=0)              | match函数的完全匹配（从字符串开头到结尾）版本                |
| findall(pattern, string, flags=0)                | 查找字符串所有与正则表达式匹配的模式 返回字符串的列表        |
| finditer(pattern, string, flags=0)               | 查找字符串所有与正则表达式匹配的模式 返回一个迭代器          |
| purge()                                          | 清除隐式编译的正则表达式的缓存                               |


> **说明：** 上面提到的re模块中的这些函数，实际开发中也可以用正则表达式对象的方法替代对这些函数的使用，如果一个正则表达式需要重复的使用，那么先通过compile函数编译正则表达式并创建出正则表达式对象无疑是更为明智的选择。

> 注：
>
> 	1. 加入  r“ ”  在前面使得字符串中转义符不起作用（对编辑器不起作用，对正则表达式正常工作）
>   2. ```
>      返回匹配字符串用group()方法
>      ```
>
>   3. search 和  match 的返回结果都可以用group()方法，而 findall  不行，group()从group(0)开始为原句本身
>
>   4. 可以在方法第三个参数指定如`re.I`，`re.DOTALL`等来忽略大小写,回车换行等指令
>
>   5. 正则表达式中空格等依然正常当字符匹配，即使用了re.DOTALL空格也不会被忽略
>
>   6. 灵活在正则表达式中加括号，用group方法获得括号里的东西,group从0开始为本身







## beautifulsoup(通常不用这个，而是用xpath或者css选择器来解析)

[官方网站文档](https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#),推荐看文档找

### find方法

```python
#find用法又很多种，下面都是

bs = BeautifulSoup(http_text, "html.parser")
# div_tag = bs.find("div")
# div_tag = bs.find(id="info-955")
# div_tag=bs.find("div",id="info-955")
# div_tag= bs.find("div",id=re.compile(r"info-\d+"))
# div_tag=bs.find(string="text")
div_tag = bs.find_all("div")
for div in div_tag:
    print(div_tag.string)
    
    
#如果find要找class属性会和关键字冲突，此时要用dict传参数，如：
bs.find("div",{"class":"name"})
```



### tag的属性

获取tag下的属性，如`<a href="http://www.baidu.com" target="_blank">`中的href或target属性有两种办法

1. ```
   # 用类似字典方法
   print(aim_txt["href"])
   ```

2. ```
   # 用.get方法
   print(aim_txt.get("href"))
   ```

> 多值属性的说明：对**class**这种多值属性会返回一个**列表**，**但如果某个属性看起来好像有多个值,却在任何版本的HTML定义中都没有被定义为多值属性**,那么Beautiful Soup会将这个属性作为**字符串**返回



### tag遍历

![Snipaste_2020-02-12_23-58-35](F:\code2\爬虫\note\src\Snipaste_2020-02-12_23-58-35.png)

>1. contents返回列表，childrens返回迭代器，两者实际内容是一样的
>2. .contents     .childrens    .previous_sibling等都会把`NavigableString`放入结果中









### NavigableString

如果没有被标签封住的内容会被识别为`NavigableString`属性,如下：

```
<body>
        这是网页的内容
        <a href="http://www.baidu.com" target="_blank">百度</a>

        <h>这里是h1内容</h>
        <h>666666666</h>
</body>
```

那么  “ \\r\\n这是网页的内容\\r\\n”    “\\r\\n              \\r\\n”    “\\r\\n”    “\\r\\n              \\r\\n”都会被.contents找出来放在列表中，其属性为`NavigableString`，没错，**请注意回车换行符的存在**









## xpath与css选择器

xpath和css在不同库中语法结构相同（而且可以在浏览器中直接复制），故推荐用这两者来作网页的解析

可以采用库lxml或者scrapy中的selector

#### 一点对比

>对于使用者来讲，哪个用的习惯就选哪个。总的来说，XPath更强大，而CSS选择器通常语法比较简洁，运行速度更快些。
>
>在项目中我们可能用的最多的是css或者xpath，那么针对这两种，我们优先选择css，原因在哪些？
>
>- 原因1：css是配合html来工作，它实现的原理是匹配对象的原理，而xpath是配合xml工作的，它实现的原理是遍历的原理，所以两者在设计上，css性能更优秀
>- 原因2：语言简洁，明了，相对xpath
>- 原因3：前段开发主要是使用css，不使用xpath，所以在技术上面，我们可以获得帮助的机会非常多
>
>题外话:据说xpath和css现在基本没有什么太大的区别了，css已经实现了大多数的xpath功能，只有个别功能没有实现。具体的数据列证还需要找更多的数据进行填充。



| Target                  | CSS 3             | XPath                                                |
| :---------------------- | :---------------- | :--------------------------------------------------- |
| 所有元素                | *                 | //*                                                  |
| 所有的P元素             | p                 | //p                                                  |
| 所有的p元素的子元素     | p > *             | //p/*                                                |
| 根据ID获取元素          | #id               | //*[@id= ‘id’]                                       |
| 根据Class获取元素       | .class            | //*[contains(@class, ‘classname’)]（**必须打引号**） |
| 拥有某个属性的元素      | *[title]          | //*[@title]                                          |
| 所有P元素的第一个子元素 | p > *:first-child | //p/*[0]                                             |
| 所有拥有子元素a的P元素  | 无法实现          | //p[a]                                               |
| 下一个兄弟元素          | P + *             | //p/following-sibling::*[0]                          |

**注：上面xpath根据class获取元素的原因是因为class可能有多个，需要用contains方法来获取，否则不会输出。用contains如果classname写到一半也会被识别获取到。**

>补：
>
>1. xpath有last方法定位最后一个元素,这是定位倒数第二个的例子         `/bookstore/book[last()-1]`
>2. xpath选取前两个元素        `/bookstore/book[position()<3]`
>3. xpath甚至元素值的阈值            `/bookstore/book[price>35.00]`
>4. xpath获取元素                   `直接在最后面@元素名称，如@class`
>5. xpath除了contains（）还有starts-with()和ends-with等强大方法
>6. xpath中括号 [ ] 里面可以用and连接两个属性
>
>
>
>1. css选择器要获取文本可以在后面加上    `::text`
>2. css中  [+和波浪线~的区别](https://blog.csdn.net/sinat_27088253/article/details/51057742)



#### 更多语法请看

> 前两个有用的网址需要看
>
> css 选择器大全https://www.w3school.com.cn/cssref/css_selectors.asp
>
> xpath 语法大全https://www.w3school.com.cn/xpath/xpath_syntax.asp
>
> 
>
> xpath 父子兄弟  https://www.jianshu.com/p/1575db75670f
>
> xpath方法（函数）大全 https://developer.mozilla.org/en-US/docs/Web/XPath/Functions





#### 代码参考

##### xpath代码参考

````python
from scrapy import selector
#也可以from scrapy import Selector (大小写有区别)
sel = selector.Selector(text=html)
# # 使用路径表达式查找，返回SelectorList对象
# tag_selector=sel.xpath("//*[@id='navbar']/div/ul/li")
# #把selectorlist变为 tag 用其.extract()方法,但是该方法生成的是一个列表,因为可能不止一个个tag(且该tag是字符串）
# tag_list=tag_selector.extract()
# tag=tag_list[0]


# 如果希望获取其中内容，则要在路径表达式最后加上/text()
# 同样的还要用.extract方法将SelectorList对象变成处理过的字符串格式
# tag_message=sel.xpath("//*[@id='navbar']/div/ul/li/text()").extract()
# print(tag_message[0])


# 还有要注意路径表达式的下标从 1 开始(确认过没有错)
# tag_message=sel.xpath("//div[@id='navbar']/div/ul/li[1]/text()").extract()


#xpath使得我们的解析可以变成可配置的解析
# name_xpath="//div[@id='navbar']/div/ul/li[1]/text()"
# tag_message=sel.xpath(name_xpath).extract()
# if tag_message:
#     print(tag_message[0])


# 指定属性选取
# href_message=sel.xpath("//a[@href='http://www.cssmoban.com/']/text()").extract()


# 输出属性而不是内容，将text()换成@属性名称
# footerP_message=sel.xpath("//a[contains(@class,'footerP')]/@class").extract()
# for info in footerP_message:
#     print(info)


# 或语句  |
footerP_and_more_message=sel.xpath("//a[contains(@class,'footerP')]/@class|//a[contains(@class,'more')]/@class").extract()
for info in footerP_and_more_message:
    print(info)

````



##### css代码参考

```python
# 和xpath类似
sel = Selector(text=html)
# footerP_selector_list=sel.css(".footerP")
# footerP_tag=footerP_selector_list.extract()


# 打印文本内容在最后面用  ::text
# footerP_text=sel.css(".footerP::text").extract()


# 选择第二个元素，nth-child 父亲的儿子，就是自己这一辈的第二个元素,返回一个列表
# footerP_text=sel.css(".footerP:nth-child(2)::text").extract()


# 选择跟在p后面的a元素
# p_a_text=sel.css("p>a::text").extract()


# 选择a前紧跟着有p的兄弟a
# p_a_text=sel.css("p+a::text").extract()


# 选择p后面的所有兄弟a元素
# p_a_text=sel.css("p~a::text").extract()


#根据别的属性选则
# p_a_text=sel.css("a[href='http://www.cssmoban.com/']::text").extract()

a_footerP_text=sel.css("a.footerP::text").extract()
for info in a_footerP_text:
    print(info)
```



